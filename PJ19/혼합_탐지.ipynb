{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae6cb09-64e2-42b8-b292-4b51e69e8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from shapely.geometry import Polygon, box\n",
    "from ultralytics import YOLO\n",
    "from glob import glob\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92e7892f-093d-470f-99a1-1b7c5e3e1056",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m image_to_save \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     17\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(image_to_save, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), (\u001b[38;5;241m950\u001b[39m, \u001b[38;5;241m955\u001b[39m), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     21\u001b[0m     area_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ultra\\lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ultra\\lib\\site-packages\\ultralytics\\engine\\model.py:555\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ultra\\lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ultra\\lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ultra\\lib\\site-packages\\ultralytics\\engine\\predictor.py:250\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 250\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ultra\\lib\\site-packages\\ultralytics\\engine\\predictor.py:129\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    126\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     im \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(im)\n\u001b[1;32m--> 129\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m im\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# uint8 to fp16/32\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_tensor:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data_dir = glob('C:/Users/ghkdr/Desktop/성현폴더/충북대/논문/데이터/혼합검증용/data/*/*.jpg')\n",
    "data_dir = glob(r'C:\\Users\\ghkdr\\Desktop\\성현폴더\\충북대\\논문\\데이터\\CH03_data2\\image_origin\\*.jpg')\n",
    "model_dir = r'C:\\Users\\ghkdr\\Desktop\\성현폴더\\충북대\\논문\\실험\\runs\\yolov9c_aug\\train2\\weights\\best.pt'\n",
    "\n",
    "save_dir = f\"result/output{len(glob('result/*'))}\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "roi = box(5, 5, 950,955)\n",
    "reference_point = (640, 480)\n",
    "model = YOLO(model_dir)\n",
    "color_list = [(255, 0, 0,), (0, 255, 0), (0, 0, 255)]\n",
    "\n",
    "for data in data_dir:\n",
    "    image = cv2.imread(data)\n",
    "    image_to_save = image.copy()\n",
    "    cv2.rectangle(image_to_save, (5, 5), (950, 955), (255, 255, 255), 3)\n",
    "    results = model(image, conf=0.8, iou=0.6, verbose=False, save=False)\n",
    "\n",
    "    for result in results:\n",
    "        area_list = []\n",
    "        bbox_list = []\n",
    "        for detection in result.boxes.xywh.cpu().numpy():\n",
    "            x, y, w, h = detection\n",
    "            bbox = box(x - w / 2, y - h / 2, x + w / 2, y + h / 2)\n",
    "            \n",
    "            if roi.contains(bbox):\n",
    "                aspect_ratio = max(w / h, h / w)\n",
    "\n",
    "                if aspect_ratio <= 1.2:\n",
    "                    # 거리 계산\n",
    "                    vector_distance = (x - reference_point[0], y - reference_point[1])\n",
    "                    scalar_distance = int(np.sqrt(vector_distance[0]**2 + vector_distance[1]**2))\n",
    "                    \n",
    "                    # 같은 (x, y) 좌표일 경우 면적을 리스트에 추가\n",
    "                    epsilon_ratio = min(((1.13 - aspect_ratio) * (w * h)) * 0.3, 0)\n",
    "                    epsilon_distance = (np.log1p(np.abs(scalar_distance) / 800) * (w * h)) * 0.03\n",
    "                    area = int(w * h) + epsilon_ratio - epsilon_distance\n",
    "                    \n",
    "                    area_list.append(area)\n",
    "                    bbox_list.append((detection))\n",
    "                    \n",
    "    # DBSCAN 클러스터링 적용\n",
    "    if len(area_list) != 0:\n",
    "        area_array = np.array(area_list).reshape(-1, 1)\n",
    "        eps = int(np.mean(area_list) * 0.25)\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=1).fit(area_array)\n",
    "        \n",
    "        dbscan.labels_\n",
    "\n",
    "        for label, bbox, area in zip(dbscan.labels_, bbox_list, area_list):\n",
    "            x, y, w, h = bbox\n",
    "            x1 = int(x - w / 2)\n",
    "            y1 = int(y - h / 2)\n",
    "            x2 = int(x + w / 2)\n",
    "            y2 = int(y + h / 2)\n",
    "            cv2.rectangle(image_to_save, (x1, y1), (x2, y2), color_list[label], 2)\n",
    "        \n",
    "            text = str(int(area))  # 여기에 표시할 텍스트\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1.5\n",
    "            font_color = (255, 0, 255) \n",
    "            thickness = 2\n",
    "            text_position = (x2 - 20, y1 - 10) \n",
    "        \n",
    "            # 텍스트를 이미지에 추가\n",
    "            cv2.putText(image_to_save, text, text_position, font, font_scale, font_color, thickness)\n",
    "\n",
    "    if len(np.unique(dbscan.labels_)) > 1:\n",
    "        file_name = data.split('\\\\')[-1]\n",
    "        cv2.imwrite(f\"{save_dir}/result_{file_name}\", image_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23115514-648f-4a9b-8822-250e9f036dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_dir = glob('C:/Users/ghkdr/Desktop/성현폴더/충북대/논문/데이터/혼합검증용/data/*/*.jpg')\n",
    "data_dir = glob(r'C:\\Users\\ghkdr\\Desktop\\성현폴더\\충북대\\논문\\데이터\\CH03_data2\\image_origin\\*.jpg')\n",
    "model_dir = r'C:\\Users\\ghkdr\\Desktop\\성현폴더\\충북대\\논문\\실험\\runs\\yolov9c_aug\\train2\\weights\\best.pt'\n",
    "\n",
    "save_dir = f\"result/output{len(glob('result/*'))}\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "roi = box(5, 5, 950,955)\n",
    "reference_point = (640, 480)\n",
    "model = YOLO(model_dir)\n",
    "color_list = [(255, 0, 0,), (0, 255, 0), (0, 0, 255)]\n",
    "\n",
    "for data in data_dir:\n",
    "    image = cv2.imread(data)\n",
    "    image_rgb = image.copy()\n",
    "    image_to_save = image.copy()\n",
    "    cv2.rectangle(image_to_save, (5, 5), (950, 955), (255, 255, 255), 3)\n",
    "    results = model(image, conf=0.8, iou=0.6, verbose=False, save=False)\n",
    "\n",
    "    for result in results:\n",
    "        area_list = []\n",
    "        bbox_list = []\n",
    "        for detection in result.boxes.xywh.cpu().numpy():\n",
    "            x, y, w, h = detection\n",
    "            bbox = box(x - w / 2, y - h / 2, x + w / 2, y + h / 2)\n",
    "            \n",
    "            if roi.contains(bbox):\n",
    "                aspect_ratio = max(w / h, h / w)\n",
    "\n",
    "                if aspect_ratio <= 1.2:\n",
    "                    # xywh를 xyxy로 변환\n",
    "                    x1, y1 = int(x - w // 2), int(y - h // 2)\n",
    "                    x2, y2 = int(x1 + w), int(y1 + h)\n",
    "\n",
    "                    # 이미지 크롭\n",
    "                    cropped_image = image_rgb[y1:y2, x1:x2].copy()\n",
    "\n",
    "                    # 가로세로 비율 계산\n",
    "                    aspect_ratio_max = max((x2 - x1) / (y2 - y1), (y2 - y1) / (x2 - x1))\n",
    "                    \n",
    "                    # 가우시안 블러 적용\n",
    "                    blurred_image = cv2.GaussianBlur(cropped_image, (11, 11), 0)\n",
    "                    \n",
    "                    # 히스토그램 평탄화 적용\n",
    "                    cropped_image_gray = cv2.cvtColor(blurred_image, cv2.COLOR_RGB2GRAY)\n",
    "                    equalized_image = cv2.equalizeHist(cropped_image_gray)\n",
    "                    \n",
    "                    # 엣지 검출\n",
    "                    edges = cv2.Canny(equalized_image, 50, 150)\n",
    "                    \n",
    "                    # 허프 원 변환을 사용한 원 검출\n",
    "                    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, dp=1.2, minDist=30, param1=190, param2=70, minRadius=30, maxRadius=5000)\n",
    "                    contour_image = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "                    if circles is not None:\n",
    "                        # 거리 계산\n",
    "                        vector_distance = (x - reference_point[0], y - reference_point[1])\n",
    "                        scalar_distance = int(np.sqrt(vector_distance[0]**2 + vector_distance[1]**2))\n",
    "                        \n",
    "                        # 같은 (x, y) 좌표일 경우 면적을 리스트에 추가\n",
    "                        epsilon_ratio = min(((1.13 - aspect_ratio) * (w * h)) * 0.3, 0)\n",
    "                        epsilon_distance = (np.log1p(np.abs(scalar_distance) / 800) * (w * h)) * 0.03\n",
    "                        area = int(w * h) + epsilon_ratio - epsilon_distance\n",
    "                        \n",
    "                        area_list.append(area)\n",
    "                        bbox_list.append((detection))\n",
    "                    \n",
    "    # DBSCAN 클러스터링 적용\n",
    "    if len(area_list) != 0:\n",
    "        area_array = np.array(area_list).reshape(-1, 1)\n",
    "        eps = int(np.mean(area_list) * 0.25)\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=1).fit(area_array)\n",
    "        \n",
    "        dbscan.labels_\n",
    "\n",
    "        for label, bbox, area in zip(dbscan.labels_, bbox_list, area_list):\n",
    "            x, y, w, h = bbox\n",
    "            x1 = int(x - w / 2)\n",
    "            y1 = int(y - h / 2)\n",
    "            x2 = int(x + w / 2)\n",
    "            y2 = int(y + h / 2)\n",
    "            cv2.rectangle(image_to_save, (x1, y1), (x2, y2), color_list[label], 2)\n",
    "        \n",
    "            text = str(int(area))  # 여기에 표시할 텍스트\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1.5\n",
    "            font_color = (255, 0, 255) \n",
    "            thickness = 2\n",
    "            text_position = (x2 - 20, y1 - 10) \n",
    "        \n",
    "            # 텍스트를 이미지에 추가\n",
    "            cv2.putText(image_to_save, text, text_position, font, font_scale, font_color, thickness)\n",
    "\n",
    "    if len(np.unique(dbscan.labels_)) > 1:\n",
    "        file_name = data.split('\\\\')[-1]\n",
    "        cv2.imwrite(f\"{save_dir}/result_{file_name}\", image_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8feff-51ae-41ca-9e92-564b155ed967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
